INFO:root:model arch:
 Transformer(
  (encoder): Encoder(
    (layers): ModuleList(
      (0): EncoderLayer(
        (self_attn): MultiHeadedAttention(
          (linears): ModuleList(
            (0): Linear(in_features=512, out_features=512, bias=True)
            (1): Linear(in_features=512, out_features=512, bias=True)
            (2): Linear(in_features=512, out_features=512, bias=True)
            (3): Linear(in_features=512, out_features=512, bias=True)
          )
          (dropout): Dropout(p=0.1, inplace=False)
        )
        (feed_forward): PositionwiseFeedForward(
          (w_1): Linear(in_features=512, out_features=1024, bias=True)
          (w_2): Linear(in_features=1024, out_features=512, bias=True)
          (dropout): Dropout(p=0.1, inplace=False)
        )
        (sublayer): ModuleList(
          (0): SublayerConnection(
            (norm): LayerNorm()
            (dropout): Dropout(p=0.1, inplace=False)
          )
          (1): SublayerConnection(
            (norm): LayerNorm()
            (dropout): Dropout(p=0.1, inplace=False)
          )
        )
      )
      (1): EncoderLayer(
        (self_attn): MultiHeadedAttention(
          (linears): ModuleList(
            (0): Linear(in_features=512, out_features=512, bias=True)
            (1): Linear(in_features=512, out_features=512, bias=True)
            (2): Linear(in_features=512, out_features=512, bias=True)
            (3): Linear(in_features=512, out_features=512, bias=True)
          )
          (dropout): Dropout(p=0.1, inplace=False)
        )
        (feed_forward): PositionwiseFeedForward(
          (w_1): Linear(in_features=512, out_features=1024, bias=True)
          (w_2): Linear(in_features=1024, out_features=512, bias=True)
          (dropout): Dropout(p=0.1, inplace=False)
        )
        (sublayer): ModuleList(
          (0): SublayerConnection(
            (norm): LayerNorm()
            (dropout): Dropout(p=0.1, inplace=False)
          )
          (1): SublayerConnection(
            (norm): LayerNorm()
            (dropout): Dropout(p=0.1, inplace=False)
          )
        )
      )
      (2): EncoderLayer(
        (self_attn): MultiHeadedAttention(
          (linears): ModuleList(
            (0): Linear(in_features=512, out_features=512, bias=True)
            (1): Linear(in_features=512, out_features=512, bias=True)
            (2): Linear(in_features=512, out_features=512, bias=True)
            (3): Linear(in_features=512, out_features=512, bias=True)
          )
          (dropout): Dropout(p=0.1, inplace=False)
        )
        (feed_forward): PositionwiseFeedForward(
          (w_1): Linear(in_features=512, out_features=1024, bias=True)
          (w_2): Linear(in_features=1024, out_features=512, bias=True)
          (dropout): Dropout(p=0.1, inplace=False)
        )
        (sublayer): ModuleList(
          (0): SublayerConnection(
            (norm): LayerNorm()
            (dropout): Dropout(p=0.1, inplace=False)
          )
          (1): SublayerConnection(
            (norm): LayerNorm()
            (dropout): Dropout(p=0.1, inplace=False)
          )
        )
      )
      (3): EncoderLayer(
        (self_attn): MultiHeadedAttention(
          (linears): ModuleList(
            (0): Linear(in_features=512, out_features=512, bias=True)
            (1): Linear(in_features=512, out_features=512, bias=True)
            (2): Linear(in_features=512, out_features=512, bias=True)
            (3): Linear(in_features=512, out_features=512, bias=True)
          )
          (dropout): Dropout(p=0.1, inplace=False)
        )
        (feed_forward): PositionwiseFeedForward(
          (w_1): Linear(in_features=512, out_features=1024, bias=True)
          (w_2): Linear(in_features=1024, out_features=512, bias=True)
          (dropout): Dropout(p=0.1, inplace=False)
        )
        (sublayer): ModuleList(
          (0): SublayerConnection(
            (norm): LayerNorm()
            (dropout): Dropout(p=0.1, inplace=False)
          )
          (1): SublayerConnection(
            (norm): LayerNorm()
            (dropout): Dropout(p=0.1, inplace=False)
          )
        )
      )
      (4): EncoderLayer(
        (self_attn): MultiHeadedAttention(
          (linears): ModuleList(
            (0): Linear(in_features=512, out_features=512, bias=True)
            (1): Linear(in_features=512, out_features=512, bias=True)
            (2): Linear(in_features=512, out_features=512, bias=True)
            (3): Linear(in_features=512, out_features=512, bias=True)
          )
          (dropout): Dropout(p=0.1, inplace=False)
        )
        (feed_forward): PositionwiseFeedForward(
          (w_1): Linear(in_features=512, out_features=1024, bias=True)
          (w_2): Linear(in_features=1024, out_features=512, bias=True)
          (dropout): Dropout(p=0.1, inplace=False)
        )
        (sublayer): ModuleList(
          (0): SublayerConnection(
            (norm): LayerNorm()
            (dropout): Dropout(p=0.1, inplace=False)
          )
          (1): SublayerConnection(
            (norm): LayerNorm()
            (dropout): Dropout(p=0.1, inplace=False)
          )
        )
      )
      (5): EncoderLayer(
        (self_attn): MultiHeadedAttention(
          (linears): ModuleList(
            (0): Linear(in_features=512, out_features=512, bias=True)
            (1): Linear(in_features=512, out_features=512, bias=True)
            (2): Linear(in_features=512, out_features=512, bias=True)
            (3): Linear(in_features=512, out_features=512, bias=True)
          )
          (dropout): Dropout(p=0.1, inplace=False)
        )
        (feed_forward): PositionwiseFeedForward(
          (w_1): Linear(in_features=512, out_features=1024, bias=True)
          (w_2): Linear(in_features=1024, out_features=512, bias=True)
          (dropout): Dropout(p=0.1, inplace=False)
        )
        (sublayer): ModuleList(
          (0): SublayerConnection(
            (norm): LayerNorm()
            (dropout): Dropout(p=0.1, inplace=False)
          )
          (1): SublayerConnection(
            (norm): LayerNorm()
            (dropout): Dropout(p=0.1, inplace=False)
          )
        )
      )
    )
    (norm): LayerNorm()
  )
  (decoder): Decoder(
    (layers): ModuleList(
      (0): DecoderLayer(
        (self_attn): MultiHeadedAttention(
          (linears): ModuleList(
            (0): Linear(in_features=512, out_features=512, bias=True)
            (1): Linear(in_features=512, out_features=512, bias=True)
            (2): Linear(in_features=512, out_features=512, bias=True)
            (3): Linear(in_features=512, out_features=512, bias=True)
          )
          (dropout): Dropout(p=0.1, inplace=False)
        )
        (src_attn): MultiHeadedAttention(
          (linears): ModuleList(
            (0): Linear(in_features=512, out_features=512, bias=True)
            (1): Linear(in_features=512, out_features=512, bias=True)
            (2): Linear(in_features=512, out_features=512, bias=True)
            (3): Linear(in_features=512, out_features=512, bias=True)
          )
          (dropout): Dropout(p=0.1, inplace=False)
        )
        (feed_forward): PositionwiseFeedForward(
          (w_1): Linear(in_features=512, out_features=1024, bias=True)
          (w_2): Linear(in_features=1024, out_features=512, bias=True)
          (dropout): Dropout(p=0.1, inplace=False)
        )
        (sublayer): ModuleList(
          (0): SublayerConnection(
            (norm): LayerNorm()
            (dropout): Dropout(p=0.1, inplace=False)
          )
          (1): SublayerConnection(
            (norm): LayerNorm()
            (dropout): Dropout(p=0.1, inplace=False)
          )
          (2): SublayerConnection(
            (norm): LayerNorm()
            (dropout): Dropout(p=0.1, inplace=False)
          )
        )
      )
      (1): DecoderLayer(
        (self_attn): MultiHeadedAttention(
          (linears): ModuleList(
            (0): Linear(in_features=512, out_features=512, bias=True)
            (1): Linear(in_features=512, out_features=512, bias=True)
            (2): Linear(in_features=512, out_features=512, bias=True)
            (3): Linear(in_features=512, out_features=512, bias=True)
          )
          (dropout): Dropout(p=0.1, inplace=False)
        )
        (src_attn): MultiHeadedAttention(
          (linears): ModuleList(
            (0): Linear(in_features=512, out_features=512, bias=True)
            (1): Linear(in_features=512, out_features=512, bias=True)
            (2): Linear(in_features=512, out_features=512, bias=True)
            (3): Linear(in_features=512, out_features=512, bias=True)
          )
          (dropout): Dropout(p=0.1, inplace=False)
        )
        (feed_forward): PositionwiseFeedForward(
          (w_1): Linear(in_features=512, out_features=1024, bias=True)
          (w_2): Linear(in_features=1024, out_features=512, bias=True)
          (dropout): Dropout(p=0.1, inplace=False)
        )
        (sublayer): ModuleList(
          (0): SublayerConnection(
            (norm): LayerNorm()
            (dropout): Dropout(p=0.1, inplace=False)
          )
          (1): SublayerConnection(
            (norm): LayerNorm()
            (dropout): Dropout(p=0.1, inplace=False)
          )
          (2): SublayerConnection(
            (norm): LayerNorm()
            (dropout): Dropout(p=0.1, inplace=False)
          )
        )
      )
      (2): DecoderLayer(
        (self_attn): MultiHeadedAttention(
          (linears): ModuleList(
            (0): Linear(in_features=512, out_features=512, bias=True)
            (1): Linear(in_features=512, out_features=512, bias=True)
            (2): Linear(in_features=512, out_features=512, bias=True)
            (3): Linear(in_features=512, out_features=512, bias=True)
          )
          (dropout): Dropout(p=0.1, inplace=False)
        )
        (src_attn): MultiHeadedAttention(
          (linears): ModuleList(
            (0): Linear(in_features=512, out_features=512, bias=True)
            (1): Linear(in_features=512, out_features=512, bias=True)
            (2): Linear(in_features=512, out_features=512, bias=True)
            (3): Linear(in_features=512, out_features=512, bias=True)
          )
          (dropout): Dropout(p=0.1, inplace=False)
        )
        (feed_forward): PositionwiseFeedForward(
          (w_1): Linear(in_features=512, out_features=1024, bias=True)
          (w_2): Linear(in_features=1024, out_features=512, bias=True)
          (dropout): Dropout(p=0.1, inplace=False)
        )
        (sublayer): ModuleList(
          (0): SublayerConnection(
            (norm): LayerNorm()
            (dropout): Dropout(p=0.1, inplace=False)
          )
          (1): SublayerConnection(
            (norm): LayerNorm()
            (dropout): Dropout(p=0.1, inplace=False)
          )
          (2): SublayerConnection(
            (norm): LayerNorm()
            (dropout): Dropout(p=0.1, inplace=False)
          )
        )
      )
      (3): DecoderLayer(
        (self_attn): MultiHeadedAttention(
          (linears): ModuleList(
            (0): Linear(in_features=512, out_features=512, bias=True)
            (1): Linear(in_features=512, out_features=512, bias=True)
            (2): Linear(in_features=512, out_features=512, bias=True)
            (3): Linear(in_features=512, out_features=512, bias=True)
          )
          (dropout): Dropout(p=0.1, inplace=False)
        )
        (src_attn): MultiHeadedAttention(
          (linears): ModuleList(
            (0): Linear(in_features=512, out_features=512, bias=True)
            (1): Linear(in_features=512, out_features=512, bias=True)
            (2): Linear(in_features=512, out_features=512, bias=True)
            (3): Linear(in_features=512, out_features=512, bias=True)
          )
          (dropout): Dropout(p=0.1, inplace=False)
        )
        (feed_forward): PositionwiseFeedForward(
          (w_1): Linear(in_features=512, out_features=1024, bias=True)
          (w_2): Linear(in_features=1024, out_features=512, bias=True)
          (dropout): Dropout(p=0.1, inplace=False)
        )
        (sublayer): ModuleList(
          (0): SublayerConnection(
            (norm): LayerNorm()
            (dropout): Dropout(p=0.1, inplace=False)
          )
          (1): SublayerConnection(
            (norm): LayerNorm()
            (dropout): Dropout(p=0.1, inplace=False)
          )
          (2): SublayerConnection(
            (norm): LayerNorm()
            (dropout): Dropout(p=0.1, inplace=False)
          )
        )
      )
      (4): DecoderLayer(
        (self_attn): MultiHeadedAttention(
          (linears): ModuleList(
            (0): Linear(in_features=512, out_features=512, bias=True)
            (1): Linear(in_features=512, out_features=512, bias=True)
            (2): Linear(in_features=512, out_features=512, bias=True)
            (3): Linear(in_features=512, out_features=512, bias=True)
          )
          (dropout): Dropout(p=0.1, inplace=False)
        )
        (src_attn): MultiHeadedAttention(
          (linears): ModuleList(
            (0): Linear(in_features=512, out_features=512, bias=True)
            (1): Linear(in_features=512, out_features=512, bias=True)
            (2): Linear(in_features=512, out_features=512, bias=True)
            (3): Linear(in_features=512, out_features=512, bias=True)
          )
          (dropout): Dropout(p=0.1, inplace=False)
        )
        (feed_forward): PositionwiseFeedForward(
          (w_1): Linear(in_features=512, out_features=1024, bias=True)
          (w_2): Linear(in_features=1024, out_features=512, bias=True)
          (dropout): Dropout(p=0.1, inplace=False)
        )
        (sublayer): ModuleList(
          (0): SublayerConnection(
            (norm): LayerNorm()
            (dropout): Dropout(p=0.1, inplace=False)
          )
          (1): SublayerConnection(
            (norm): LayerNorm()
            (dropout): Dropout(p=0.1, inplace=False)
          )
          (2): SublayerConnection(
            (norm): LayerNorm()
            (dropout): Dropout(p=0.1, inplace=False)
          )
        )
      )
      (5): DecoderLayer(
        (self_attn): MultiHeadedAttention(
          (linears): ModuleList(
            (0): Linear(in_features=512, out_features=512, bias=True)
            (1): Linear(in_features=512, out_features=512, bias=True)
            (2): Linear(in_features=512, out_features=512, bias=True)
            (3): Linear(in_features=512, out_features=512, bias=True)
          )
          (dropout): Dropout(p=0.1, inplace=False)
        )
        (src_attn): MultiHeadedAttention(
          (linears): ModuleList(
            (0): Linear(in_features=512, out_features=512, bias=True)
            (1): Linear(in_features=512, out_features=512, bias=True)
            (2): Linear(in_features=512, out_features=512, bias=True)
            (3): Linear(in_features=512, out_features=512, bias=True)
          )
          (dropout): Dropout(p=0.1, inplace=False)
        )
        (feed_forward): PositionwiseFeedForward(
          (w_1): Linear(in_features=512, out_features=1024, bias=True)
          (w_2): Linear(in_features=1024, out_features=512, bias=True)
          (dropout): Dropout(p=0.1, inplace=False)
        )
        (sublayer): ModuleList(
          (0): SublayerConnection(
            (norm): LayerNorm()
            (dropout): Dropout(p=0.1, inplace=False)
          )
          (1): SublayerConnection(
            (norm): LayerNorm()
            (dropout): Dropout(p=0.1, inplace=False)
          )
          (2): SublayerConnection(
            (norm): LayerNorm()
            (dropout): Dropout(p=0.1, inplace=False)
          )
        )
      )
    )
    (norm): LayerNorm()
  )
  (src_embed): Sequential(
    (0): Embeddings(
      (lut): Embedding(1435, 512)
    )
    (1): PositionalEncoding(
      (dropout): Dropout(p=0.1, inplace=False)
    )
  )
  (tgt_embed): Sequential(
    (0): Embeddings(
      (lut): Embedding(1237, 512)
    )
    (1): PositionalEncoding(
      (dropout): Dropout(p=0.1, inplace=False)
    )
  )
  (generator): Generator(
    (proj): Linear(in_features=512, out_features=1237, bias=True)
  )
)
INFO:root:epoch: 0, train_loss: 0.0196
INFO:root:epoch: 1, train_loss: 0.0169
INFO:root:epoch: 2, train_loss: 0.0157
INFO:root:epoch: 3, train_loss: 0.0147
INFO:root:epoch: 4, train_loss: 0.0138
INFO:root:epoch: 5, train_loss: 0.0131
INFO:root:epoch: 6, train_loss: 0.0125
INFO:root:epoch: 7, train_loss: 0.0116
INFO:root:epoch: 8, train_loss: 0.0108
INFO:root:epoch: 9, train_loss: 0.0101
INFO:root:epoch: 10, train_loss: 0.0093
INFO:root:epoch: 11, train_loss: 0.0086
INFO:root:epoch: 12, train_loss: 0.0078
INFO:root:epoch: 13, train_loss: 0.0072
INFO:root:epoch: 14, train_loss: 0.0065
INFO:root:epoch: 15, train_loss: 0.0059
INFO:root:epoch: 16, train_loss: 0.0052
INFO:root:epoch: 17, train_loss: 0.0046
INFO:root:epoch: 18, train_loss: 0.0041
INFO:root:epoch: 19, train_loss: 0.0037
INFO:root:epoch: 20, train_loss: 0.0031
INFO:root:epoch: 21, train_loss: 0.0026
INFO:root:epoch: 22, train_loss: 0.0023
INFO:root:epoch: 23, train_loss: 0.0020
INFO:root:epoch: 24, train_loss: 0.0015
INFO:root:epoch: 25, train_loss: 0.0013
INFO:root:epoch: 26, train_loss: 0.0011
INFO:root:epoch: 27, train_loss: 0.0010
INFO:root:epoch: 28, train_loss: 0.0009
INFO:root:epoch: 29, train_loss: 0.0008
INFO:root:epoch: 30, train_loss: 0.0007
INFO:root:epoch: 31, train_loss: 0.0007
INFO:root:epoch: 32, train_loss: 0.0006
INFO:root:epoch: 33, train_loss: 0.0006
INFO:root:epoch: 34, train_loss: 0.0006
INFO:root:epoch: 35, train_loss: 0.0006
INFO:root:epoch: 36, train_loss: 0.0006
INFO:root:epoch: 37, train_loss: 0.0005
INFO:root:epoch: 38, train_loss: 0.0005
INFO:root:epoch: 39, train_loss: 0.0005
INFO:root:epoch: 40, train_loss: 0.0005
INFO:root:epoch: 41, train_loss: 0.0005
INFO:root:epoch: 42, train_loss: 0.0005
INFO:root:epoch: 43, train_loss: 0.0005
INFO:root:epoch: 44, train_loss: 0.0004
INFO:root:epoch: 45, train_loss: 0.0005
INFO:root:epoch: 46, train_loss: 0.0005
INFO:root:epoch: 47, train_loss: 0.0005
INFO:root:epoch: 48, train_loss: 0.0004
INFO:root:epoch: 49, train_loss: 0.0004
INFO:root:epoch: 50, train_loss: 0.0004
INFO:root:epoch: 51, train_loss: 0.0004
INFO:root:epoch: 52, train_loss: 0.0005
INFO:root:epoch: 53, train_loss: 0.0005
INFO:root:epoch: 54, train_loss: 0.0005
INFO:root:epoch: 55, train_loss: 0.0004
INFO:root:epoch: 56, train_loss: 0.0005
INFO:root:epoch: 57, train_loss: 0.0005
INFO:root:epoch: 58, train_loss: 0.0004
INFO:root:epoch: 59, train_loss: 0.0004
INFO:root:epoch: 60, train_loss: 0.0005
INFO:root:epoch: 61, train_loss: 0.0004
INFO:root:epoch: 62, train_loss: 0.0004
INFO:root:epoch: 63, train_loss: 0.0004
INFO:root:epoch: 64, train_loss: 0.0004
INFO:root:epoch: 65, train_loss: 0.0004
INFO:root:epoch: 66, train_loss: 0.0004
INFO:root:epoch: 67, train_loss: 0.0004
INFO:root:epoch: 68, train_loss: 0.0004
INFO:root:epoch: 69, train_loss: 0.0004
INFO:root:epoch: 70, train_loss: 0.0003
INFO:root:epoch: 71, train_loss: 0.0003
INFO:root:epoch: 72, train_loss: 0.0003
INFO:root:epoch: 73, train_loss: 0.0003
INFO:root:epoch: 74, train_loss: 0.0003
INFO:root:epoch: 75, train_loss: 0.0003
INFO:root:epoch: 76, train_loss: 0.0003
INFO:root:epoch: 77, train_loss: 0.0003
INFO:root:epoch: 78, train_loss: 0.0003
INFO:root:epoch: 79, train_loss: 0.0003
INFO:root:epoch: 80, train_loss: 0.0002
INFO:root:epoch: 81, train_loss: 0.0002
INFO:root:epoch: 82, train_loss: 0.0003
INFO:root:epoch: 83, train_loss: 0.0002
INFO:root:epoch: 84, train_loss: 0.0002
INFO:root:epoch: 85, train_loss: 0.0002
INFO:root:epoch: 86, train_loss: 0.0002
INFO:root:epoch: 87, train_loss: 0.0002
INFO:root:epoch: 88, train_loss: 0.0002
INFO:root:epoch: 89, train_loss: 0.0002
INFO:root:epoch: 90, train_loss: 0.0002
INFO:root:epoch: 91, train_loss: 0.0002
INFO:root:epoch: 92, train_loss: 0.0002
INFO:root:epoch: 93, train_loss: 0.0002
INFO:root:epoch: 94, train_loss: 0.0002
INFO:root:epoch: 95, train_loss: 0.0002
INFO:root:epoch: 96, train_loss: 0.0002
INFO:root:epoch: 97, train_loss: 0.0001
INFO:root:epoch: 98, train_loss: 0.0002
INFO:root:epoch: 99, train_loss: 0.0001
INFO:root:epoch: 100, train_loss: 0.0002
INFO:root:= == == == == =SAVE MODEL= = = = = = = = = = 
INFO:root:epoch: 101, train_loss: 0.0002
INFO:root:epoch: 102, train_loss: 0.0001
INFO:root:epoch: 103, train_loss: 0.0001
INFO:root:epoch: 104, train_loss: 0.0002
INFO:root:epoch: 105, train_loss: 0.0001
INFO:root:epoch: 106, train_loss: 0.0001
INFO:root:epoch: 107, train_loss: 0.0001
INFO:root:epoch: 108, train_loss: 0.0001
INFO:root:epoch: 109, train_loss: 0.0001
INFO:root:epoch: 110, train_loss: 0.0001
INFO:root:epoch: 111, train_loss: 0.0001
INFO:root:epoch: 112, train_loss: 0.0001
INFO:root:epoch: 113, train_loss: 0.0001
INFO:root:epoch: 114, train_loss: 0.0001
INFO:root:epoch: 115, train_loss: 0.0001
INFO:root:epoch: 116, train_loss: 0.0001
INFO:root:epoch: 117, train_loss: 0.0001
INFO:root:epoch: 118, train_loss: 0.0001
INFO:root:epoch: 119, train_loss: 0.0001
INFO:root:epoch: 120, train_loss: 0.0001
INFO:root:epoch: 121, train_loss: 0.0001
INFO:root:epoch: 122, train_loss: 0.0001
INFO:root:epoch: 123, train_loss: 0.0001
INFO:root:epoch: 124, train_loss: 0.0001
INFO:root:epoch: 125, train_loss: 0.0001
INFO:root:epoch: 126, train_loss: 0.0001
INFO:root:epoch: 127, train_loss: 0.0001
INFO:root:epoch: 128, train_loss: 0.0001
INFO:root:epoch: 129, train_loss: 0.0001
INFO:root:epoch: 130, train_loss: 0.0001
INFO:root:epoch: 131, train_loss: 0.0001
INFO:root:epoch: 132, train_loss: 0.0001
INFO:root:epoch: 133, train_loss: 0.0001
INFO:root:epoch: 134, train_loss: 0.0001
INFO:root:epoch: 135, train_loss: 0.0001
INFO:root:epoch: 136, train_loss: 0.0001
INFO:root:epoch: 137, train_loss: 0.0001
INFO:root:epoch: 138, train_loss: 0.0001
INFO:root:epoch: 139, train_loss: 0.0001
INFO:root:epoch: 140, train_loss: 0.0001
INFO:root:epoch: 141, train_loss: 0.0001
INFO:root:epoch: 142, train_loss: 0.0001
INFO:root:epoch: 143, train_loss: 0.0001
INFO:root:epoch: 144, train_loss: 0.0001
INFO:root:epoch: 145, train_loss: 0.0001
INFO:root:epoch: 146, train_loss: 0.0001
INFO:root:epoch: 147, train_loss: 0.0001
INFO:root:epoch: 148, train_loss: 0.0001
INFO:root:epoch: 149, train_loss: 0.0001
INFO:root:epoch: 150, train_loss: 0.0001
INFO:root:epoch: 151, train_loss: 0.0001
INFO:root:epoch: 152, train_loss: 0.0001
INFO:root:epoch: 153, train_loss: 0.0001
INFO:root:epoch: 154, train_loss: 0.0001
INFO:root:epoch: 155, train_loss: 0.0001
INFO:root:epoch: 156, train_loss: 0.0001
INFO:root:epoch: 157, train_loss: 0.0001
INFO:root:epoch: 158, train_loss: 0.0001
INFO:root:epoch: 159, train_loss: 0.0001
INFO:root:epoch: 160, train_loss: 0.0001
INFO:root:epoch: 161, train_loss: 0.0001
INFO:root:epoch: 162, train_loss: 0.0001
INFO:root:epoch: 163, train_loss: 0.0001
INFO:root:epoch: 164, train_loss: 0.0001
INFO:root:epoch: 165, train_loss: 0.0001
INFO:root:epoch: 166, train_loss: 0.0001
INFO:root:epoch: 167, train_loss: 0.0001
INFO:root:epoch: 168, train_loss: 0.0001
INFO:root:epoch: 169, train_loss: 0.0001
INFO:root:epoch: 170, train_loss: 0.0001
INFO:root:epoch: 171, train_loss: 0.0001
INFO:root:epoch: 172, train_loss: 0.0001
INFO:root:epoch: 173, train_loss: 0.0001
INFO:root:epoch: 174, train_loss: 0.0001
INFO:root:epoch: 175, train_loss: 0.0001
INFO:root:epoch: 176, train_loss: 0.0000
INFO:root:epoch: 177, train_loss: 0.0000
INFO:root:epoch: 178, train_loss: 0.0001
INFO:root:epoch: 179, train_loss: 0.0001
INFO:root:epoch: 180, train_loss: 0.0000
INFO:root:epoch: 181, train_loss: 0.0000
INFO:root:epoch: 182, train_loss: 0.0001
INFO:root:epoch: 183, train_loss: 0.0000
INFO:root:epoch: 184, train_loss: 0.0001
INFO:root:epoch: 185, train_loss: 0.0001
INFO:root:epoch: 186, train_loss: 0.0001
INFO:root:epoch: 187, train_loss: 0.0000
INFO:root:epoch: 188, train_loss: 0.0000
INFO:root:epoch: 189, train_loss: 0.0000
INFO:root:epoch: 190, train_loss: 0.0001
INFO:root:epoch: 191, train_loss: 0.0001
INFO:root:epoch: 192, train_loss: 0.0000
INFO:root:epoch: 193, train_loss: 0.0000
INFO:root:epoch: 194, train_loss: 0.0000
INFO:root:epoch: 195, train_loss: 0.0000
INFO:root:epoch: 196, train_loss: 0.0001
INFO:root:epoch: 197, train_loss: 0.0001
INFO:root:epoch: 198, train_loss: 0.0000
INFO:root:epoch: 199, train_loss: 0.0000
INFO:root:epoch: 200, train_loss: 0.0000
INFO:root:= == == == == =SAVE MODEL= = = = = = = = = = 
INFO:root:epoch: 201, train_loss: 0.0000
INFO:root:epoch: 202, train_loss: 0.0000
INFO:root:epoch: 203, train_loss: 0.0000
INFO:root:epoch: 204, train_loss: 0.0000
INFO:root:epoch: 205, train_loss: 0.0000
INFO:root:epoch: 206, train_loss: 0.0001
INFO:root:epoch: 207, train_loss: 0.0001
INFO:root:epoch: 208, train_loss: 0.0001
INFO:root:epoch: 209, train_loss: 0.0000
INFO:root:epoch: 210, train_loss: 0.0000
INFO:root:epoch: 211, train_loss: 0.0000
INFO:root:epoch: 212, train_loss: 0.0000
INFO:root:epoch: 213, train_loss: 0.0000
INFO:root:epoch: 214, train_loss: 0.0000
INFO:root:epoch: 215, train_loss: 0.0000
INFO:root:epoch: 216, train_loss: 0.0001
INFO:root:epoch: 217, train_loss: 0.0000
INFO:root:epoch: 218, train_loss: 0.0000
INFO:root:epoch: 219, train_loss: 0.0000
INFO:root:epoch: 220, train_loss: 0.0000
INFO:root:epoch: 221, train_loss: 0.0000
INFO:root:epoch: 222, train_loss: 0.0000
INFO:root:epoch: 223, train_loss: 0.0000
INFO:root:epoch: 224, train_loss: 0.0000
INFO:root:epoch: 225, train_loss: 0.0000
INFO:root:epoch: 226, train_loss: 0.0000
INFO:root:epoch: 227, train_loss: 0.0000
INFO:root:epoch: 228, train_loss: 0.0000
INFO:root:epoch: 229, train_loss: 0.0000
INFO:root:epoch: 230, train_loss: 0.0000
INFO:root:epoch: 231, train_loss: 0.0000
INFO:root:epoch: 232, train_loss: 0.0000
INFO:root:epoch: 233, train_loss: 0.0001
INFO:root:epoch: 234, train_loss: 0.0000
INFO:root:epoch: 235, train_loss: 0.0000
INFO:root:epoch: 236, train_loss: 0.0000
INFO:root:epoch: 237, train_loss: 0.0000
INFO:root:epoch: 238, train_loss: 0.0000
INFO:root:epoch: 239, train_loss: 0.0000
INFO:root:epoch: 240, train_loss: 0.0000
INFO:root:epoch: 241, train_loss: 0.0001
INFO:root:epoch: 242, train_loss: 0.0000
INFO:root:epoch: 243, train_loss: 0.0000
INFO:root:epoch: 244, train_loss: 0.0000
INFO:root:epoch: 245, train_loss: 0.0000
INFO:root:epoch: 246, train_loss: 0.0000
INFO:root:epoch: 247, train_loss: 0.0000
INFO:root:epoch: 248, train_loss: 0.0000
INFO:root:epoch: 249, train_loss: 0.0000
INFO:root:epoch: 250, train_loss: 0.0000
INFO:root:epoch: 251, train_loss: 0.0000
INFO:root:epoch: 252, train_loss: 0.0000
INFO:root:epoch: 253, train_loss: 0.0000
INFO:root:epoch: 254, train_loss: 0.0000
INFO:root:epoch: 255, train_loss: 0.0000
INFO:root:epoch: 256, train_loss: 0.0000
INFO:root:epoch: 257, train_loss: 0.0000
INFO:root:epoch: 258, train_loss: 0.0000
INFO:root:epoch: 259, train_loss: 0.0000
INFO:root:epoch: 260, train_loss: 0.0000
INFO:root:epoch: 261, train_loss: 0.0000
INFO:root:epoch: 262, train_loss: 0.0000
INFO:root:epoch: 263, train_loss: 0.0000
INFO:root:epoch: 264, train_loss: 0.0000
INFO:root:epoch: 265, train_loss: 0.0000
INFO:root:epoch: 266, train_loss: 0.0000
INFO:root:epoch: 267, train_loss: 0.0000
INFO:root:epoch: 268, train_loss: 0.0000
INFO:root:epoch: 269, train_loss: 0.0000
INFO:root:epoch: 270, train_loss: 0.0000
INFO:root:epoch: 271, train_loss: 0.0000
INFO:root:epoch: 272, train_loss: 0.0000
INFO:root:epoch: 273, train_loss: 0.0000
INFO:root:epoch: 274, train_loss: 0.0000
INFO:root:epoch: 275, train_loss: 0.0000
INFO:root:epoch: 276, train_loss: 0.0000
INFO:root:epoch: 277, train_loss: 0.0000
INFO:root:epoch: 278, train_loss: 0.0000
INFO:root:epoch: 279, train_loss: 0.0000
INFO:root:epoch: 280, train_loss: 0.0000
INFO:root:epoch: 281, train_loss: 0.0000
INFO:root:epoch: 282, train_loss: 0.0000
INFO:root:epoch: 283, train_loss: 0.0000
INFO:root:epoch: 284, train_loss: 0.0000
INFO:root:epoch: 285, train_loss: 0.0000
INFO:root:epoch: 286, train_loss: 0.0000
